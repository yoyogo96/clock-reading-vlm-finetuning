#!/usr/bin/env python3
"""
Train English Clock VLM
ì˜ì–´ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹œê³„ ì½ê¸° VLM íŒŒì¸íŠœë‹
"""

import torch
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from english_dataset import EnglishClockDataModule
from tqdm import tqdm
import json
import time

def train_english_clock_model():
    """ì˜ì–´ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹œê³„ ëª¨ë¸ í•™ìŠµ"""
    
    print("ğŸš€ ì˜ì–´ ì‹œê³„ ì½ê¸° ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    print("=" * 50)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if torch.backends.mps.is_available():
        device = torch.device("mps")
        print("âœ… Mac GPU (MPS) ì‚¬ìš©")
    else:
        device = torch.device("cpu")
        print("ğŸ’» CPU ì‚¬ìš©")
    
    # ëª¨ë¸ ë¡œë“œ (ìƒˆë¡œìš´ ë² ì´ìŠ¤ë¼ì¸ë¶€í„° ì‹œì‘)
    model_name = "Salesforce/blip2-opt-2.7b"
    print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”©: {model_name}")
    
    processor = Blip2Processor.from_pretrained(model_name)
    model = Blip2ForConditionalGeneration.from_pretrained(
        model_name,
        torch_dtype=torch.float32 if device.type == "mps" else torch.float16
    ).to(device)
    
    print(f"Model loaded with {sum(p.numel() for p in model.parameters())} parameters")
    
    # ì˜ì–´ ë°ì´í„° ì„¤ì •
    data_module = EnglishClockDataModule(
        data_dir="dataset",
        processor=processor,
        batch_size=4,  # ë” ì•ˆì •ì ì¸ ë°°ì¹˜ í¬ê¸°
        num_workers=0,  # Macì—ì„œ ì•ˆì •ì 
        reasoning_mode=True
    )
    data_module.setup()
    
    train_loader = data_module.train_dataloader()
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)
    
    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    num_epochs = 1
    steps_per_epoch = 20  # ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´
    
    print(f"\nğŸ¯ í•™ìŠµ ì„¤ì •:")
    print(f"  ì—í¬í¬: {num_epochs}")
    print(f"  ìŠ¤í…/ì—í¬í¬: {steps_per_epoch}")
    print(f"  ì´ ìŠ¤í…: {num_epochs * steps_per_epoch}")
    print(f"  í•™ìŠµë¥ : 5e-6")
    
    # í•™ìŠµ ì‹œì‘
    model.train()
    all_losses = []
    
    print(f"\nğŸ“š ì˜ì–´ í•™ìŠµ ì‹œì‘...")
    
    for epoch in range(num_epochs):
        print(f"\n=== Epoch {epoch+1}/{num_epochs} ===")
        
        train_iter = iter(train_loader)
        epoch_losses = []
        
        for step in range(steps_per_epoch):
            try:
                batch = next(train_iter)
            except StopIteration:
                train_iter = iter(train_loader)
                batch = next(train_iter)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            batch['pixel_values'] = batch['pixel_values'].to(device)
            batch['input_ids'] = batch['input_ids'].to(device)
            batch['attention_mask'] = batch['attention_mask'].to(device)
            batch['labels'] = batch['labels'].to(device)
            
            optimizer.zero_grad()
            
            # Forward pass
            outputs = model(
                pixel_values=batch['pixel_values'],
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask'],
                labels=batch['labels']
            )
            
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            
            loss_value = loss.item()
            epoch_losses.append(loss_value)
            all_losses.append(loss_value)
            
            print(f"  Step {step+1}/{steps_per_epoch}: Loss = {loss_value:.4f}")
            
            # ìƒ˜í”Œ ì •ë³´ ì¶œë ¥ (ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ)
            if step == 0:
                print(f"    ìƒ˜í”Œ ì˜ˆì‹œ: {batch['answer_texts'][0]}")
        
        epoch_avg_loss = sum(epoch_losses) / len(epoch_losses)
        print(f"  Epoch {epoch+1} í‰ê·  ì†ì‹¤: {epoch_avg_loss:.4f}")
    
    # ëª¨ë¸ ì €ì¥
    print(f"\nğŸ’¾ ì˜ì–´ ëª¨ë¸ ì €ì¥...")
    model.save_pretrained("./english_finetuned_clock_model")
    processor.save_pretrained("./english_finetuned_clock_model")
    
    # í•™ìŠµ ê²°ê³¼ ì €ì¥
    training_results = {
        'model_name': model_name,
        'dataset': 'English Clock Dataset',
        'device': str(device),
        'num_epochs': num_epochs,
        'steps_per_epoch': steps_per_epoch,
        'total_steps': len(all_losses),
        'losses': all_losses,
        'final_loss': all_losses[-1],
        'initial_loss': all_losses[0],
        'loss_reduction': all_losses[0] - all_losses[-1],
        'training_date': '2025-06-21'
    }
    
    with open('english_training_results.json', 'w', encoding='utf-8') as f:
        json.dump(training_results, f, ensure_ascii=False, indent=2)
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š ì˜ì–´ í•™ìŠµ ê²°ê³¼:")
    print(f"  ì´ í•™ìŠµ ìŠ¤í…: {len(all_losses)}")
    print(f"  ì´ˆê¸° ì†ì‹¤: {all_losses[0]:.4f}")
    print(f"  ìµœì¢… ì†ì‹¤: {all_losses[-1]:.4f}")
    print(f"  ì†ì‹¤ ê°ì†Œ: {all_losses[0] - all_losses[-1]:.4f}")
    
    if all_losses[-1] < all_losses[0]:
        print(f"âœ… í•™ìŠµ ì„±ê³µ: ì†ì‹¤ ê°ì†Œ í™•ì¸!")
    else:
        print(f"âš ï¸ ì¶”ê°€ í•™ìŠµ í•„ìš”")
    
    print(f"\nğŸ‰ ì˜ì–´ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼:")
    print(f"  ğŸ¤– ./english_finetuned_clock_model/ - ì˜ì–´ íŒŒì¸íŠœë‹ ëª¨ë¸")
    print(f"  ğŸ“„ english_training_results.json - í•™ìŠµ ë¡œê·¸")
    
    return training_results

if __name__ == "__main__":
    results = train_english_clock_model()